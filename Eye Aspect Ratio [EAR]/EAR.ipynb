{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Face Mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, min_detection_confidence=0.5)\n",
    "\n",
    "# Initialize OpenCV video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Constants and counters for EAR and MAR\n",
    "EAR_CONSEC_FRAMES = 30\n",
    "EAR_ALERT_THRESHOLD = 5\n",
    "ALERT_DISPLAY_FRAMES = 60\n",
    "\n",
    "# Variables for tracking the state\n",
    "ear_frame_count = 0\n",
    "ear_alert_count = 0\n",
    "alert_frame_counter = 0\n",
    "display_alert = False\n",
    "\n",
    "# FPS calculation\n",
    "prev_time = time.time()\n",
    "\n",
    "# EAR calculation function\n",
    "def calculate_ear(landmarks, eye_indices):\n",
    "    left_point = np.array([landmarks[eye_indices[0]].x, landmarks[eye_indices[0]].y])\n",
    "    right_point = np.array([landmarks[eye_indices[3]].x, landmarks[eye_indices[3]].y])\n",
    "    top_mid = np.array([(landmarks[eye_indices[1]].x + landmarks[eye_indices[2]].x) / 2,\n",
    "                        (landmarks[eye_indices[1]].y + landmarks[eye_indices[2]].y) / 2])\n",
    "    bottom_mid = np.array([(landmarks[eye_indices[4]].x + landmarks[eye_indices[5]].x) / 2,\n",
    "                           (landmarks[eye_indices[4]].y + landmarks[eye_indices[5]].y) / 2])\n",
    "\n",
    "    horizontal_distance = np.linalg.norm(right_point - left_point)\n",
    "    vertical_distance = np.linalg.norm(top_mid - bottom_mid)\n",
    "\n",
    "    ear = vertical_distance / horizontal_distance\n",
    "    return ear\n",
    "\n",
    "\n",
    "# Calculate brightness\n",
    "def calculate_brightness(frame):\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    return np.mean(gray_frame)\n",
    "\n",
    "# Adjust EAR threshold based on brightness\n",
    "def adjust_ear_threshold(brightness):\n",
    "    if brightness < 50:\n",
    "        return 0.18\n",
    "    elif brightness < 100:\n",
    "        return 0.21\n",
    "    else:\n",
    "        return 0.24\n",
    "    \n",
    "# Indices for left and right eyes  (MediaPipe Face Mesh)\n",
    "LEFT_EYE_INDICES = [33, 160, 158, 133, 153, 144]\n",
    "RIGHT_EYE_INDICES = [362, 385, 387, 263, 373, 380]\n",
    "\n",
    "\n",
    "try:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame. Exiting...\")\n",
    "            break\n",
    "\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(rgb_frame)\n",
    "\n",
    "        h, w, _ = frame.shape\n",
    "\n",
    "        # Calculate brightness and EAR threshold\n",
    "        brightness = calculate_brightness(frame)\n",
    "        ear_threshold = adjust_ear_threshold(brightness)\n",
    "\n",
    "        # FPS calculation\n",
    "        current_time = time.time()\n",
    "        fps = 1 / (current_time - prev_time)\n",
    "        prev_time = current_time\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                landmarks = face_landmarks.landmark\n",
    "                \n",
    "                # Draw circles on left and right eye landmarks\n",
    "                for idx in LEFT_EYE_INDICES + RIGHT_EYE_INDICES:\n",
    "                    x = int(landmarks[idx].x * w)\n",
    "                    y = int(landmarks[idx].y * h)\n",
    "                    cv2.circle(frame, (x, y), 2, (0, 200, 200), -1)\n",
    "\n",
    "                # Calculate EAR and MAR\n",
    "                left_ear = calculate_ear(landmarks, LEFT_EYE_INDICES)\n",
    "                right_ear = calculate_ear(landmarks, RIGHT_EYE_INDICES)\n",
    "                avg_ear = (left_ear + right_ear) / 2\n",
    "\n",
    "                # EAR detection logic\n",
    "                if avg_ear < ear_threshold:\n",
    "                    ear_frame_count += 1\n",
    "                    if ear_frame_count == EAR_CONSEC_FRAMES:\n",
    "                        ear_alert_count += 1\n",
    "                        ear_frame_count = 0\n",
    "                else:\n",
    "                    ear_frame_count = 0\n",
    "                    \n",
    "        # Trigger final alert if thresholds are reached\n",
    "        if ear_alert_count >= EAR_ALERT_THRESHOLD:\n",
    "            alert_frame_counter = ALERT_DISPLAY_FRAMES\n",
    "            ear_alert_count = 0\n",
    "            display_alert = True\n",
    "            \n",
    "        # Decrement alert frame counter\n",
    "        if alert_frame_counter > 0:\n",
    "            alert_frame_counter -= 1\n",
    "        else:\n",
    "            display_alert = False\n",
    "            \n",
    "        # Draw semi-transparent box with a small black border for overlay information\n",
    "        overlay = frame.copy()\n",
    "        cv2.rectangle(overlay, (5, 5), (180, 100), (96, 96, 96), -1)  # Semi-transparent box\n",
    "        alpha = 0.6\n",
    "        frame = cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0)  # Blend overlay with frame\n",
    "        cv2.rectangle(frame, (5, 5), (180, 100), (0, 0, 0), 2)  # Black border\n",
    "        \n",
    "         # Display information on frame\n",
    "        cv2.putText(frame, f\"EAR COUNT: {ear_frame_count}\", (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "        \n",
    "        cv2.putText(frame, f\"EAR ALERTS: {ear_alert_count}\", (15, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "    \n",
    "        cv2.putText(frame, f\"FPS: {fps:.2f}\", (15, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "\n",
    "        if display_alert:\n",
    "            cv2.putText(frame, \"ALERT: DROWSINESS DETECTED!\", (w // 2 - 250, h // 2), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "\n",
    "        # Resize the frame to fit the window\n",
    "        frame = cv2.resize(frame, (780, 540))  # Set to the same size as the window\n",
    "        cv2.imshow(\"Drowsiness Detection\", frame)\n",
    "        cv2.namedWindow(\"Drowsiness Detection\", cv2.WINDOW_NORMAL)\n",
    "        cv2.resizeWindow(\"Drowsiness Detection\", 780, 540)  # Set the window size (width x height)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
